{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a5bed93",
   "metadata": {},
   "source": [
    "#### RAG exampls from scratch to End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "971b8eb2-54ca-4b16-b046-079d526b406e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.document_loaders import WebBaseLoader,ArxivLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # ! pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain cohere\n",
    "# # pip install 'protobuf<=3.20.1' --force-reinstall\n",
    "# # !python3 -m pip install pip --upgrade\n",
    "# # !pip install pyopenssl --upgrade\n",
    "# !pip install pymupdf\n",
    "# !pip install langchain-cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a66c5fd1-8da9-416f-b854-1b70d207d606",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = \"\"\n",
    "\n",
    "os.environ['COHERE_API_KEY'] = \"\"\n",
    "os.environ['OPENAI_API_KEY'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d28a4df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Paid\n",
    "# llm= ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "# embeddings=OpenAIEmbeddings()\n",
    "\n",
    "### Free\n",
    "llm = Ollama(model=\"llama3\")\n",
    "embeddings=CohereEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bea008f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Modular RAG is an architecture that advances beyond previous RAG paradigms by offering enhanced adaptability and versatility through the introduction of new modules and patterns. It builds upon the foundational principles of Advanced and Naive RAG while allowing for module substitution or reconfiguration to address specific challenges, improving retrieval processes and information relevance. The shift towards a modular RAG approach supports both sequential processing and integrated end-to-end training across its components.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# Step 1: Load the documents\n",
    "docs = ArxivLoader(query='2312.10997', load_max_docs=1, load_all_available_meta=True).load()\n",
    "load_docs = docs[0].page_content\n",
    "\n",
    "# Step 2: Filter complex metadata\n",
    "filtered_docs = filter_complex_metadata(docs)\n",
    "\n",
    "# Step 3: Split the documents\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=300, chunk_overlap=50)\n",
    "splits = text_splitter.split_documents(filtered_docs)\n",
    "\n",
    "# Step 4: Embed the documents\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
    "\n",
    "# Step 5: Create the retriever\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs , \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Question\n",
    "rag_chain.invoke(\"What is Modular RAG ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238ca125",
   "metadata": {},
   "source": [
    "How to write prompts- templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba8ac6b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template='You are a Q&A assistant, You will refer the context provided and answer the question. \\nIf you dont know the answer , reply that you dont know the answer:\\n{context}\\nQuestion: {question}\\n'))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prompt\n",
    "template = \"\"\"You are a Q&A assistant, You will refer the context provided and answer the question. \n",
    "If you dont know the answer , reply that you dont know the answer:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8fdb1d",
   "metadata": {},
   "source": [
    "### Tweaking RAG : Work on queries, Work on prompts , work with rerankers & then see what you have got !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59482e5",
   "metadata": {},
   "source": [
    "#### Multiquery Perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95e76bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comprehensive approach not only streamlines the retrieval pro-\n",
      "cess but also significantly improves the quality and relevance\n",
      "of the information retrieved, catering to a wide array of tasks\n",
      "and queries with enhanced precision and flexibility.\n",
      "2) New Patterns: Modular RAG offers remarkable adapt-\n",
      "ability by allowing module substitution or reconfiguration\n",
      "to address specific challenges. This goes beyond the fixed\n",
      "structures of Naive and Advanced RAG, characterized by a\n",
      "simple “Retrieve” and “Read” mechanism. Moreover, Modular\n",
      "RAG expands this flexibility by integrating new modules or\n",
      "adjusting interaction flow among existing ones, enhancing its\n",
      "applicability across different tasks.\n",
      "Innovations such as the Rewrite-Retrieve-Read [7]model\n",
      "leverage the LLM’s capabilities to refine retrieval queries\n",
      "through a rewriting module and a LM-feedback mechanism\n",
      "to update rewriting model., improving task performance.\n",
      "Similarly, approaches like Generate-Read [13] replace tradi-\n",
      "tional retrieval with LLM-generated content, while Recite-\n",
      "Read [22] emphasizes retrieval from model weights, enhanc-\n",
      "ing the model’s ability to handle knowledge-intensive tasks.\n",
      "Hybrid retrieval strategies integrate keyword, semantic, and\n",
      "vector searches to cater to diverse queries. Additionally, em-\n",
      "ploying sub-queries and hypothetical document embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Satej Raste\\anaconda3\\lib\\site-packages\\langchain_core\\_api\\beta_decorator.py:87: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  warn_beta(\n"
     ]
    }
   ],
   "source": [
    "# Multi Query: Different Perspectives\n",
    "template = \"\"\"You are an AI language model assistant. \n",
    "Your task is to generate five different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. \n",
    "By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: \n",
    "{question}\"\"\"\n",
    "\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_perspectives \n",
    "    | llm \n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "from langchain.load import dumps, loads\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "# Retrieve\n",
    "question = \"What is Modular RAG ?\"\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "docs = retrieval_chain.invoke({\"question\":question})\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bcf9768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Modular RAG is an advanced approach that goes beyond the traditional Naive and Advanced RAG paradigms by offering enhanced adaptability and versatility. It incorporates diverse strategies for improving its components, such as adding specialized modules for retrieval and processing capabilities, allowing for module substitution or reconfiguration to address specific challenges. Modular RAG builds upon the foundational principles of Advanced and Naive RAG while introducing new modules and innovative techniques to enhance retrieval and generation processes.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RAG\n",
    "from operator import itemgetter\n",
    "\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac28af7",
   "metadata": {},
   "source": [
    " You can see our anwer depends on the answers which were coming out of the multiquery retriever chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0406721",
   "metadata": {},
   "source": [
    "#### Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a367b583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. What are the components of RAG analysis?',\n",
       " '2. How does RAG analysis differ from Advanced RAG analysis?',\n",
       " '3. Can you provide a detailed explanation of RAG analysis for better understanding?']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Decomposition\n",
    "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (3 queries):\"\"\"\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)\n",
    "# Chain\n",
    "generate_queries_decomposition = ( prompt_decomposition | llm | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n",
    "\n",
    "# Run\n",
    "question = \"I dont understand RAG ,Can you help me understand what are the components and one more thing I would like to know about whether is it same as Advanced RAG ?\"\n",
    "#### I gave an ambigious query which talks about 3 questions 1. RAG understanding 2. Components of RAG 3. Difference between RAG & Advanced RAG\n",
    "questions = generate_queries_decomposition.invoke({\"question\":question})\n",
    "\n",
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c175fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---\n",
      "Question: 1. What are the components of RAG analysis?\n",
      "Answer: The components of RAG analysis include retrieval quality evaluation, generation quality assessment, and augmentation processes. These components focus on measuring the effectiveness of the context sourced by the retriever component, the generator's ability to synthesize coherent and relevant answers, and the enhancement of generated content through augmentation techniques. Additionally, evaluation benchmarks and tools are used to quantify the performance of RAG models across these components.\n",
      "---\n",
      "Question: 2. How does RAG analysis differ from Advanced RAG analysis?\n",
      "Answer: RAG analysis differs from Advanced RAG analysis in terms of specific improvements introduced in Advanced RAG to overcome the limitations of Naive RAG. Advanced RAG focuses on enhancing retrieval quality by employing pre-retrieval and post-retrieval strategies. It refines its indexing techniques through the use of a sliding window approach, fine-grained segmentation, and the incorporation of metadata. Additionally, Advanced RAG incorporates several optimization methods to streamline the retrieval process. This shows that Advanced RAG goes beyond the basic components of RAG analysis to address specific challenges and enhance the overall performance of the retrieval-augmented generation process.\n",
      "---\n",
      "Question: 3. Can you provide a detailed explanation of RAG analysis for better understanding?\n",
      "Answer: RAG analysis involves evaluating the performance of Retrieval-Augmented Generation (RAG) models across different aspects to ensure their effectiveness. The analysis consists of three main components: retrieval quality evaluation, generation quality assessment, and augmentation processes.\n",
      "\n",
      "1. **Retrieval Quality Evaluation**: This aspect focuses on assessing the effectiveness of the context sourced by the retriever component in the RAG model. Metrics commonly used for evaluating retrieval quality include Hit Rate, Mean Reciprocal Rank (MRR), and Normalized Discounted Cumulative Gain (NDCG). These metrics help measure how well the retriever component retrieves relevant information from external databases.\n",
      "\n",
      "2. **Generation Quality Assessment**: The generation quality assessment component evaluates the generator's ability to synthesize coherent and relevant answers based on the retrieved context. The assessment can be categorized into unlabeled and labeled content evaluation. For unlabeled content, factors like faithfulness, relevance, and non-harmfulness of the generated answers are considered. In contrast, for labeled content, the focus is on the accuracy of the information produced by the model.\n",
      "\n",
      "3. **Augmentation Processes**: The augmentation processes component focuses on enhancing the generated content through augmentation techniques. These techniques aim to improve the overall quality and relevance of the generated answers by incorporating additional information or refining the existing content.\n",
      "\n",
      "In addition to these components, RAG analysis also involves the use of evaluation benchmarks and tools to quantify the performance of RAG models. Benchmarks such as RGB, RECALL, and CRUD, along with automated tools like RAGAS, ARES, and TruLens, provide quantitative metrics to assess the capabilities of RAG models across various evaluation aspects.\n",
      "\n",
      "Overall, RAG analysis aims to provide a comprehensive evaluation framework for assessing the performance of RAG models, ensuring their accuracy, credibility, and effectiveness in generating relevant and coherent answers for knowledge-intensive tasks.\n"
     ]
    }
   ],
   "source": [
    "# Prompt\n",
    "template = \"\"\"Here is the question you need to answer:\n",
    "\n",
    "\\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "Here is any available background question + answer pairs:\n",
    "\n",
    "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "Here is additional context relevant to the question: \n",
    "\n",
    "\\n --- \\n {context} \\n --- \\n\n",
    "\n",
    "Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
    "\"\"\"\n",
    "\n",
    "decomposition_prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def format_qa_pair(question, answer):\n",
    "    \"\"\"Format Q and A pair\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    formatted_string += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "\n",
    "\n",
    "q_a_pairs = \"\"\n",
    "for q in questions:\n",
    "    \n",
    "    rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | retriever, \n",
    "     \"question\": itemgetter(\"question\"),\n",
    "     \"q_a_pairs\": itemgetter(\"q_a_pairs\")} \n",
    "    | decomposition_prompt\n",
    "    | llm\n",
    "    | StrOutputParser())\n",
    "\n",
    "    answer = rag_chain.invoke({\"question\":q,\"q_a_pairs\":q_a_pairs})\n",
    "    q_a_pair = format_qa_pair(q,answer)\n",
    "    q_a_pairs = q_a_pairs + \"\\n---\\n\"+  q_a_pair\n",
    "    \n",
    "print(q_a_pairs)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4fa2d3",
   "metadata": {},
   "source": [
    "It answered all the questions one by one Now its time to look at the actual query and revert it in a single answer.Lets print it and see what happens !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d193e7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG analysis involves evaluating the performance of Retrieval-Augmented Generation (RAG) models across different aspects to ensure their effectiveness. The analysis consists of three main components: retrieval quality evaluation, generation quality assessment, and augmentation processes.\n",
      "\n",
      "1. **Retrieval Quality Evaluation**: This aspect focuses on assessing the effectiveness of the context sourced by the retriever component in the RAG model. Metrics commonly used for evaluating retrieval quality include Hit Rate, Mean Reciprocal Rank (MRR), and Normalized Discounted Cumulative Gain (NDCG). These metrics help measure how well the retriever component retrieves relevant information from external databases.\n",
      "\n",
      "2. **Generation Quality Assessment**: The generation quality assessment component evaluates the generator's ability to synthesize coherent and relevant answers based on the retrieved context. The assessment can be categorized into unlabeled and labeled content evaluation. For unlabeled content, factors like faithfulness, relevance, and non-harmfulness of the generated answers are considered. In contrast, for labeled content, the focus is on the accuracy of the information produced by the model.\n",
      "\n",
      "3. **Augmentation Processes**: The augmentation processes component focuses on enhancing the generated content through augmentation techniques. These techniques aim to improve the overall quality and relevance of the generated answers by incorporating additional information or refining the existing content.\n",
      "\n",
      "In addition to these components, RAG analysis also involves the use of evaluation benchmarks and tools to quantify the performance of RAG models. Benchmarks such as RGB, RECALL, and CRUD, along with automated tools like RAGAS, ARES, and TruLens, provide quantitative metrics to assess the capabilities of RAG models across various evaluation aspects.\n",
      "\n",
      "Overall, RAG analysis aims to provide a comprehensive evaluation framework for assessing the performance of RAG models, ensuring their accuracy, credibility, and effectiveness in generating relevant and coherent answers for knowledge-intensive tasks.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fc736b",
   "metadata": {},
   "source": [
    "### HYDE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "030681e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RAG, which stands for Retrieval-Augmented Generation, is a technology that enhances the capabilities of Large Language Models (LLMs) by integrating knowledge from external databases. The components of RAG include retrieval, generation, and augmentation techniques. The retrieval component sources context from external databases, the generation component synthesizes coherent and relevant answers based on the retrieved context, and the augmentation component enhances the generated content with additional information.\\n\\nRegarding your question about whether RAG is the same as Advanced RAG, it is important to note that Advanced RAG is a specific developmental paradigm within the RAG framework. RAG encompasses different stages of development, including Naive RAG, Advanced RAG, and Modular RAG. Advanced RAG represents a progressive enhancement over its predecessors, showcasing advancements in the technology and capabilities of RAG models. So, while Advanced RAG is a part of the broader RAG framework, it is not the same as RAG itself.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# HyDE document genration\n",
    "template = \"\"\"Please write a scientific paper passage to answer the question\n",
    "Question: {question}\n",
    "Passage:\"\"\"\n",
    "prompt_hyde = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "generate_docs_for_retrieval = (\n",
    "    prompt_hyde | llm | StrOutputParser() \n",
    ")\n",
    "\n",
    "# Run\n",
    "generate_docs_for_retrieval.invoke({\"question\":question})\n",
    "# Retrieve\n",
    "retrieval_chain = generate_docs_for_retrieval | retriever \n",
    "retireved_docs = retrieval_chain.invoke({\"question\":question})\n",
    "retireved_docs\n",
    "\n",
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"context\":retireved_docs,\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3c1aa8",
   "metadata": {},
   "source": [
    "### RAG Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f201eec1-6ed0-4236-9594-286894574779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(Document(page_content='comprehensive approach not only streamlines the retrieval pro-\\ncess but also significantly improves the quality and relevance\\nof the information retrieved, catering to a wide array of tasks\\nand queries with enhanced precision and flexibility.\\n2) New Patterns: Modular RAG offers remarkable adapt-\\nability by allowing module substitution or reconfiguration\\nto address specific challenges. This goes beyond the fixed\\nstructures of Naive and Advanced RAG, characterized by a\\nsimple “Retrieve” and “Read” mechanism. Moreover, Modular\\nRAG expands this flexibility by integrating new modules or\\nadjusting interaction flow among existing ones, enhancing its\\napplicability across different tasks.\\nInnovations such as the Rewrite-Retrieve-Read [7]model\\nleverage the LLM’s capabilities to refine retrieval queries\\nthrough a rewriting module and a LM-feedback mechanism\\nto update rewriting model., improving task performance.\\nSimilarly, approaches like Generate-Read [13] replace tradi-\\ntional retrieval with LLM-generated content, while Recite-\\nRead [22] emphasizes retrieval from model weights, enhanc-\\ning the model’s ability to handle knowledge-intensive tasks.\\nHybrid retrieval strategies integrate keyword, semantic, and\\nvector searches to cater to diverse queries. Additionally, em-\\nploying sub-queries and hypothetical document embeddings', metadata={'Authors': 'Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, Haofen Wang', 'Published': '2024-03-27', 'Summary': \"Large Language Models (LLMs) showcase impressive capabilities but encounter\\nchallenges like hallucination, outdated knowledge, and non-transparent,\\nuntraceable reasoning processes. Retrieval-Augmented Generation (RAG) has\\nemerged as a promising solution by incorporating knowledge from external\\ndatabases. This enhances the accuracy and credibility of the generation,\\nparticularly for knowledge-intensive tasks, and allows for continuous knowledge\\nupdates and integration of domain-specific information. RAG synergistically\\nmerges LLMs' intrinsic knowledge with the vast, dynamic repositories of\\nexternal databases. This comprehensive review paper offers a detailed\\nexamination of the progression of RAG paradigms, encompassing the Naive RAG,\\nthe Advanced RAG, and the Modular RAG. It meticulously scrutinizes the\\ntripartite foundation of RAG frameworks, which includes the retrieval, the\\ngeneration and the augmentation techniques. The paper highlights the\\nstate-of-the-art technologies embedded in each of these critical components,\\nproviding a profound understanding of the advancements in RAG systems.\\nFurthermore, this paper introduces up-to-date evaluation framework and\\nbenchmark. At the end, this article delineates the challenges currently faced\\nand points out prospective avenues for research and development.\", 'Title': 'Retrieval-Augmented Generation for Large Language Models: A Survey', 'comment': 'Ongoing Work', 'entry_id': 'http://arxiv.org/abs/2312.10997v5', 'primary_category': 'cs.CL', 'published_first_time': '2023-12-18'}), 0.04972677595628415), (Document(page_content='3https://www.langchain.com/\\nC. Modular RAG\\nThe modular RAG architecture advances beyond the for-\\nmer two RAG paradigms, offering enhanced adaptability and\\nversatility. It incorporates diverse strategies for improving its\\ncomponents, such as adding a search module for similarity\\nsearches and refining the retriever through fine-tuning. Inno-\\nvations like restructured RAG modules [13] and rearranged\\nRAG pipelines [14] have been introduced to tackle specific\\nchallenges. The shift towards a modular RAG approach is\\nbecoming prevalent, supporting both sequential processing and\\nintegrated end-to-end training across its components. Despite\\nits distinctiveness, Modular RAG builds upon the foundational\\nprinciples of Advanced and Naive RAG, illustrating a progres-\\nsion and refinement within the RAG family.\\n1) New Modules: The Modular RAG framework introduces\\nadditional specialized components to enhance retrieval and\\nprocessing capabilities. The Search module adapts to spe-\\ncific scenarios, enabling direct searches across various data\\nsources like search engines, databases, and knowledge graphs,\\nusing LLM-generated code and query languages [15]. RAG-\\nFusion addresses traditional search limitations by employing\\na multi-query strategy that expands user queries into diverse\\nperspectives, utilizing parallel vector searches and intelligent', metadata={'Authors': 'Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, Haofen Wang', 'Published': '2024-03-27', 'Summary': \"Large Language Models (LLMs) showcase impressive capabilities but encounter\\nchallenges like hallucination, outdated knowledge, and non-transparent,\\nuntraceable reasoning processes. Retrieval-Augmented Generation (RAG) has\\nemerged as a promising solution by incorporating knowledge from external\\ndatabases. This enhances the accuracy and credibility of the generation,\\nparticularly for knowledge-intensive tasks, and allows for continuous knowledge\\nupdates and integration of domain-specific information. RAG synergistically\\nmerges LLMs' intrinsic knowledge with the vast, dynamic repositories of\\nexternal databases. This comprehensive review paper offers a detailed\\nexamination of the progression of RAG paradigms, encompassing the Naive RAG,\\nthe Advanced RAG, and the Modular RAG. It meticulously scrutinizes the\\ntripartite foundation of RAG frameworks, which includes the retrieval, the\\ngeneration and the augmentation techniques. The paper highlights the\\nstate-of-the-art technologies embedded in each of these critical components,\\nproviding a profound understanding of the advancements in RAG systems.\\nFurthermore, this paper introduces up-to-date evaluation framework and\\nbenchmark. At the end, this article delineates the challenges currently faced\\nand points out prospective avenues for research and development.\", 'Title': 'Retrieval-Augmented Generation for Large Language Models: A Survey', 'comment': 'Ongoing Work', 'entry_id': 'http://arxiv.org/abs/2312.10997v5', 'primary_category': 'cs.CL', 'published_first_time': '2023-12-18'}), 0.04945355191256831), (Document(page_content='enhancements to the technology stack drive the development\\nof RAG capabilities. RAG toolkits are converging into a\\nfoundational technology stack, laying the groundwork for\\nadvanced enterprise applications. However, a fully integrated,\\ncomprehensive platform concept is still in the future, requiring\\nfurther innovation and development.\\nF. Multi-modal RAG\\nRAG\\nhas\\ntranscended\\nits\\ninitial\\ntext-based\\nquestion-\\nanswering confines, embracing a diverse array of modal data.\\nThis expansion has spawned innovative multimodal models\\nthat integrate RAG concepts across various domains:\\nImage. RA-CM3 [176] stands as a pioneering multimodal\\nmodel of both retrieving and generating text and images.\\nBLIP-2 [177] leverages frozen image encoders alongside\\nLLMs for efficient visual language pre-training, enabling zero-\\nshot image-to-text conversions. The “Visualize Before You\\nWrite” method [178] employs image generation to steer the\\nLM’s text generation, showing promise in open-ended text\\ngeneration tasks.\\nAudio and Video. The GSS method retrieves and stitches\\ntogether audio clips to convert machine-translated data into\\nspeech-translated data [179]. UEOP marks a significant ad-\\nvancement in end-to-end automatic speech recognition by', metadata={'Authors': 'Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, Haofen Wang', 'Published': '2024-03-27', 'Summary': \"Large Language Models (LLMs) showcase impressive capabilities but encounter\\nchallenges like hallucination, outdated knowledge, and non-transparent,\\nuntraceable reasoning processes. Retrieval-Augmented Generation (RAG) has\\nemerged as a promising solution by incorporating knowledge from external\\ndatabases. This enhances the accuracy and credibility of the generation,\\nparticularly for knowledge-intensive tasks, and allows for continuous knowledge\\nupdates and integration of domain-specific information. RAG synergistically\\nmerges LLMs' intrinsic knowledge with the vast, dynamic repositories of\\nexternal databases. This comprehensive review paper offers a detailed\\nexamination of the progression of RAG paradigms, encompassing the Naive RAG,\\nthe Advanced RAG, and the Modular RAG. It meticulously scrutinizes the\\ntripartite foundation of RAG frameworks, which includes the retrieval, the\\ngeneration and the augmentation techniques. The paper highlights the\\nstate-of-the-art technologies embedded in each of these critical components,\\nproviding a profound understanding of the advancements in RAG systems.\\nFurthermore, this paper introduces up-to-date evaluation framework and\\nbenchmark. At the end, this article delineates the challenges currently faced\\nand points out prospective avenues for research and development.\", 'Title': 'Retrieval-Augmented Generation for Large Language Models: A Survey', 'comment': 'Ongoing Work', 'entry_id': 'http://arxiv.org/abs/2312.10997v5', 'primary_category': 'cs.CL', 'published_first_time': '2023-12-18'}), 0.047619047619047616), (Document(page_content='Hybrid retrieval strategies integrate keyword, semantic, and\\nvector searches to cater to diverse queries. Additionally, em-\\nploying sub-queries and hypothetical document embeddings\\n(HyDE) [11] seeks to improve retrieval relevance by focusing\\non embedding similarities between generated answers and real\\ndocuments.\\nAdjustments in module arrangement and interaction, such\\nas the Demonstrate-Search-Predict (DSP) [23] framework\\nand the iterative Retrieve-Read-Retrieve-Read flow of ITER-\\nRETGEN [14], showcase the dynamic use of module out-\\nputs to bolster another module’s functionality, illustrating a\\nsophisticated understanding of enhancing module synergy.\\nThe flexible orchestration of Modular RAG Flow showcases\\nthe benefits of adaptive retrieval through techniques such as\\nFLARE [24] and Self-RAG [25]. This approach transcends\\nthe fixed RAG retrieval process by evaluating the necessity\\nof retrieval based on different scenarios. Another benefit of\\na flexible architecture is that the RAG system can more\\neasily integrate with other technologies (such as fine-tuning\\nor reinforcement learning) [26]. For example, this can involve\\nfine-tuning the retriever for better retrieval results, fine-tuning\\nthe generator for more personalized outputs, or engaging in\\ncollaborative fine-tuning [27].', metadata={'Authors': 'Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, Haofen Wang', 'Published': '2024-03-27', 'Summary': \"Large Language Models (LLMs) showcase impressive capabilities but encounter\\nchallenges like hallucination, outdated knowledge, and non-transparent,\\nuntraceable reasoning processes. Retrieval-Augmented Generation (RAG) has\\nemerged as a promising solution by incorporating knowledge from external\\ndatabases. This enhances the accuracy and credibility of the generation,\\nparticularly for knowledge-intensive tasks, and allows for continuous knowledge\\nupdates and integration of domain-specific information. RAG synergistically\\nmerges LLMs' intrinsic knowledge with the vast, dynamic repositories of\\nexternal databases. This comprehensive review paper offers a detailed\\nexamination of the progression of RAG paradigms, encompassing the Naive RAG,\\nthe Advanced RAG, and the Modular RAG. It meticulously scrutinizes the\\ntripartite foundation of RAG frameworks, which includes the retrieval, the\\ngeneration and the augmentation techniques. The paper highlights the\\nstate-of-the-art technologies embedded in each of these critical components,\\nproviding a profound understanding of the advancements in RAG systems.\\nFurthermore, this paper introduces up-to-date evaluation framework and\\nbenchmark. At the end, this article delineates the challenges currently faced\\nand points out prospective avenues for research and development.\", 'Title': 'Retrieval-Augmented Generation for Large Language Models: A Survey', 'comment': 'Ongoing Work', 'entry_id': 'http://arxiv.org/abs/2312.10997v5', 'primary_category': 'cs.CL', 'published_first_time': '2023-12-18'}), 0.03225806451612903), (Document(page_content='The summary of this paper, as depicted in Figure 6, empha-\\nsizes RAG’s significant advancement in enhancing the capa-\\nbilities of LLMs by integrating parameterized knowledge from\\nlanguage models with extensive non-parameterized data from\\nexternal knowledge bases. The survey showcases the evolution\\nof RAG technologies and their application on many different\\ntasks. The analysis outlines three developmental paradigms\\nwithin the RAG framework: Naive, Advanced, and Modu-\\nlar RAG, each representing a progressive enhancement over\\nits predecessors. RAG’s technical integration with other AI\\nmethodologies, such as fine-tuning and reinforcement learning,\\nhas further expanded its capabilities. Despite the progress in\\nRAG technology, there are research opportunities to improve\\nits robustness and its ability to handle extended contexts.\\nRAG’s application scope is expanding into multimodal do-\\nmains, adapting its principles to interpret and process diverse\\ndata forms like images, videos, and code. This expansion high-\\nlights RAG’s significant practical implications for AI deploy-\\nment, attracting interest from academic and industrial sectors.\\n17\\nThe growing ecosystem of RAG is evidenced by the rise in\\nRAG-centric AI applications and the continuous development\\nof supportive tools. As RAG’s application landscape broadens,\\nthere is a need to refine evaluation methodologies to keep', metadata={'Authors': 'Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, Haofen Wang', 'Published': '2024-03-27', 'Summary': \"Large Language Models (LLMs) showcase impressive capabilities but encounter\\nchallenges like hallucination, outdated knowledge, and non-transparent,\\nuntraceable reasoning processes. Retrieval-Augmented Generation (RAG) has\\nemerged as a promising solution by incorporating knowledge from external\\ndatabases. This enhances the accuracy and credibility of the generation,\\nparticularly for knowledge-intensive tasks, and allows for continuous knowledge\\nupdates and integration of domain-specific information. RAG synergistically\\nmerges LLMs' intrinsic knowledge with the vast, dynamic repositories of\\nexternal databases. This comprehensive review paper offers a detailed\\nexamination of the progression of RAG paradigms, encompassing the Naive RAG,\\nthe Advanced RAG, and the Modular RAG. It meticulously scrutinizes the\\ntripartite foundation of RAG frameworks, which includes the retrieval, the\\ngeneration and the augmentation techniques. The paper highlights the\\nstate-of-the-art technologies embedded in each of these critical components,\\nproviding a profound understanding of the advancements in RAG systems.\\nFurthermore, this paper introduces up-to-date evaluation framework and\\nbenchmark. At the end, this article delineates the challenges currently faced\\nand points out prospective avenues for research and development.\", 'Title': 'Retrieval-Augmented Generation for Large Language Models: A Survey', 'comment': 'Ongoing Work', 'entry_id': 'http://arxiv.org/abs/2312.10997v5', 'primary_category': 'cs.CL', 'published_first_time': '2023-12-18'}), 0.016129032258064516)]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# RAG-Fusion\n",
    "template = \"\"\"You are an assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (3 queries):\n",
    "\"\"\"\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_rag_fusion \n",
    "    | llm\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "from langchain.load import dumps, loads\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents \n",
    "        and an optional parameter k used in the RRF formula \"\"\"\n",
    "    \n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
    "            doc_str = dumps(doc)\n",
    "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Retrieve the current score of the document, if any\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results\n",
    "\n",
    "question = \"What is pattern in Modular RAG ?\"\n",
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "\n",
    "print(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee43bb94-613d-46a3-b688-02c2d2aadfba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pattern in Modular RAG refers to the adaptability and flexibility offered by allowing module substitution or reconfiguration to address specific challenges, going beyond the fixed structures of Naive and Advanced RAG.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# RAG\n",
    "template = \"\"\"\n",
    "Answer the following question based on this context, \n",
    "If you dont find any answer then just revert with 'Answer not found'.\n",
    "context: {context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "#llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain_rag_fusion, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f21b53e",
   "metadata": {},
   "source": [
    "##### Lets look at the Advanced RAG : Using CohereReranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b88e1ad8-57eb-40d4-8475-a50775a692b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pattern in Modular RAG refers to the adaptability and flexibility of the framework by allowing module substitution or reconfiguration to address specific challenges. It goes beyond the fixed structures of Naive and Advanced RAG, allowing for the integration of new modules or adjusting interaction flow among existing ones to enhance its applicability across different tasks.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import Cohere\n",
    "from langchain.retrievers import  ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CohereRerank\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 25})\n",
    "\n",
    "# Chain\n",
    "normal_rag_chain = (\n",
    "    {\"context\": retriever | format_docs , \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "# Question\n",
    "normal_rag_chain.invoke(\"What is pattern in Modular RAG ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "637a669a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Satej Raste\\anaconda3\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `CohereRerank` was deprecated in LangChain 0.0.30 and will be removed in 0.3.0. An updated version of the class exists in the langchain-cohere package and should be used instead. To use it run `pip install -U langchain-cohere` and import as `from langchain_cohere import CohereRerank`.\n",
      "  warn_deprecated(\n",
      "C:\\Users\\Satej Raste\\anaconda3\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer after reranking comes out to be: \n",
      "Pattern in Modular RAG is the Rewrite-Retrieve-Read model, Generate-Read approach, and Recite-Read strategy.\n",
      "\n",
      "Retrieved Documents:\n",
      "\n",
      "Document 1:\n",
      "comprehensive approach not only streamlines the retrieval pro-\n",
      "cess but also significantly improves the quality and relevance\n",
      "of the information retrieved, catering to a wide array of tasks\n",
      "and queries with enhanced precision and flexibility.\n",
      "2) New Patterns: Modular RAG offers remarkable adapt-\n",
      "ability by allowing module substitution or reconfiguration\n",
      "to address specific challenges. This goes beyond the fixed\n",
      "structures of Naive and Advanced RAG, characterized by a\n",
      "simple “Retrieve” and “Read” mechanism. Moreover, Modular\n",
      "RAG expands this flexibility by integrating new modules or\n",
      "adjusting interaction flow among existing ones, enhancing its\n",
      "applicability across different tasks.\n",
      "Innovations such as the Rewrite-Retrieve-Read [7]model\n",
      "leverage the LLM’s capabilities to refine retrieval queries\n",
      "through a rewriting module and a LM-feedback mechanism\n",
      "to update rewriting model., improving task performance.\n",
      "Similarly, approaches like Generate-Read [13] replace tradi-\n",
      "tional retrieval with LLM-generated content, while Recite-\n",
      "Read [22] emphasizes retrieval from model weights, enhanc-\n",
      "ing the model’s ability to handle knowledge-intensive tasks.\n",
      "Hybrid retrieval strategies integrate keyword, semantic, and\n",
      "vector searches to cater to diverse queries. Additionally, em-\n",
      "ploying sub-queries and hypothetical document embeddings\n",
      "\n",
      "Document 2:\n",
      "comprehensive approach not only streamlines the retrieval pro-\n",
      "cess but also significantly improves the quality and relevance\n",
      "of the information retrieved, catering to a wide array of tasks\n",
      "and queries with enhanced precision and flexibility.\n",
      "2) New Patterns: Modular RAG offers remarkable adapt-\n",
      "ability by allowing module substitution or reconfiguration\n",
      "to address specific challenges. This goes beyond the fixed\n",
      "structures of Naive and Advanced RAG, characterized by a\n",
      "simple “Retrieve” and “Read” mechanism. Moreover, Modular\n",
      "RAG expands this flexibility by integrating new modules or\n",
      "adjusting interaction flow among existing ones, enhancing its\n",
      "applicability across different tasks.\n",
      "Innovations such as the Rewrite-Retrieve-Read [7]model\n",
      "leverage the LLM’s capabilities to refine retrieval queries\n",
      "through a rewriting module and a LM-feedback mechanism\n",
      "to update rewriting model., improving task performance.\n",
      "Similarly, approaches like Generate-Read [13] replace tradi-\n",
      "tional retrieval with LLM-generated content, while Recite-\n",
      "Read [22] emphasizes retrieval from model weights, enhanc-\n",
      "ing the model’s ability to handle knowledge-intensive tasks.\n",
      "Hybrid retrieval strategies integrate keyword, semantic, and\n",
      "vector searches to cater to diverse queries. Additionally, em-\n",
      "ploying sub-queries and hypothetical document embeddings\n",
      "\n",
      "Document 3:\n",
      "comprehensive approach not only streamlines the retrieval pro-\n",
      "cess but also significantly improves the quality and relevance\n",
      "of the information retrieved, catering to a wide array of tasks\n",
      "and queries with enhanced precision and flexibility.\n",
      "2) New Patterns: Modular RAG offers remarkable adapt-\n",
      "ability by allowing module substitution or reconfiguration\n",
      "to address specific challenges. This goes beyond the fixed\n",
      "structures of Naive and Advanced RAG, characterized by a\n",
      "simple “Retrieve” and “Read” mechanism. Moreover, Modular\n",
      "RAG expands this flexibility by integrating new modules or\n",
      "adjusting interaction flow among existing ones, enhancing its\n",
      "applicability across different tasks.\n",
      "Innovations such as the Rewrite-Retrieve-Read [7]model\n",
      "leverage the LLM’s capabilities to refine retrieval queries\n",
      "through a rewriting module and a LM-feedback mechanism\n",
      "to update rewriting model., improving task performance.\n",
      "Similarly, approaches like Generate-Read [13] replace tradi-\n",
      "tional retrieval with LLM-generated content, while Recite-\n",
      "Read [22] emphasizes retrieval from model weights, enhanc-\n",
      "ing the model’s ability to handle knowledge-intensive tasks.\n",
      "Hybrid retrieval strategies integrate keyword, semantic, and\n",
      "vector searches to cater to diverse queries. Additionally, em-\n",
      "ploying sub-queries and hypothetical document embeddings\n",
      "\n",
      "Document 4:\n",
      "comprehensive approach not only streamlines the retrieval pro-\n",
      "cess but also significantly improves the quality and relevance\n",
      "of the information retrieved, catering to a wide array of tasks\n",
      "and queries with enhanced precision and flexibility.\n",
      "2) New Patterns: Modular RAG offers remarkable adapt-\n",
      "ability by allowing module substitution or reconfiguration\n",
      "to address specific challenges. This goes beyond the fixed\n",
      "structures of Naive and Advanced RAG, characterized by a\n",
      "simple “Retrieve” and “Read” mechanism. Moreover, Modular\n",
      "RAG expands this flexibility by integrating new modules or\n",
      "adjusting interaction flow among existing ones, enhancing its\n",
      "applicability across different tasks.\n",
      "Innovations such as the Rewrite-Retrieve-Read [7]model\n",
      "leverage the LLM’s capabilities to refine retrieval queries\n",
      "through a rewriting module and a LM-feedback mechanism\n",
      "to update rewriting model., improving task performance.\n",
      "Similarly, approaches like Generate-Read [13] replace tradi-\n",
      "tional retrieval with LLM-generated content, while Recite-\n",
      "Read [22] emphasizes retrieval from model weights, enhanc-\n",
      "ing the model’s ability to handle knowledge-intensive tasks.\n",
      "Hybrid retrieval strategies integrate keyword, semantic, and\n",
      "vector searches to cater to diverse queries. Additionally, em-\n",
      "ploying sub-queries and hypothetical document embeddings\n",
      "\n",
      "Document 5:\n",
      "comprehensive approach not only streamlines the retrieval pro-\n",
      "cess but also significantly improves the quality and relevance\n",
      "of the information retrieved, catering to a wide array of tasks\n",
      "and queries with enhanced precision and flexibility.\n",
      "2) New Patterns: Modular RAG offers remarkable adapt-\n",
      "ability by allowing module substitution or reconfiguration\n",
      "to address specific challenges. This goes beyond the fixed\n",
      "structures of Naive and Advanced RAG, characterized by a\n",
      "simple “Retrieve” and “Read” mechanism. Moreover, Modular\n",
      "RAG expands this flexibility by integrating new modules or\n",
      "adjusting interaction flow among existing ones, enhancing its\n",
      "applicability across different tasks.\n",
      "Innovations such as the Rewrite-Retrieve-Read [7]model\n",
      "leverage the LLM’s capabilities to refine retrieval queries\n",
      "through a rewriting module and a LM-feedback mechanism\n",
      "to update rewriting model., improving task performance.\n",
      "Similarly, approaches like Generate-Read [13] replace tradi-\n",
      "tional retrieval with LLM-generated content, while Recite-\n",
      "Read [22] emphasizes retrieval from model weights, enhanc-\n",
      "ing the model’s ability to handle knowledge-intensive tasks.\n",
      "Hybrid retrieval strategies integrate keyword, semantic, and\n",
      "vector searches to cater to diverse queries. Additionally, em-\n",
      "ploying sub-queries and hypothetical document embeddings\n"
     ]
    }
   ],
   "source": [
    "# Re-rank\n",
    "top_k=5\n",
    "compressor = CohereRerank(top_n=top_k)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")\n",
    "question=\"What is pattern in Modular RAG ?\"\n",
    "compressed_docs = compression_retriever.get_relevant_documents(question)\n",
    "\n",
    "#### After using reranker\n",
    "reranked_rag_chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"Answer after reranking comes out to be: \")\n",
    "print(reranked_rag_chain.invoke({\"context\":compressed_docs,\"question\":question}))\n",
    "\n",
    "\n",
    "# The retrieved source documents\n",
    "print(\"\\nRetrieved Documents:\")\n",
    "for i in range(top_k):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(compressed_docs[0].page_content)  # or doc.text depending on the document structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771bebec",
   "metadata": {},
   "source": [
    "#### You see how answer changes once you make use of additional retriever !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4b9175",
   "metadata": {},
   "source": [
    "### Advanced RAG Cohere Reranker(): How reranking looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "878fed01",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### RAG & Applying Cohere Reranker for document extraction\n",
    "import fitz  # PyMuPDF\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "\n",
    "from langchain import hub\n",
    "\n",
    "# # Loads the latest version\n",
    "# prompt = hub.pull(\"rlm/rag-prompt\", api_url=\"https://api.hub.langchain.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aad9255a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Satej Raste\\anaconda3\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Modular RAG introduces additional specialized components to enhance retrieval and processing capabilities. Some of the key components include:\n",
      "\n",
      "1) Search Module: This module adapts to specific scenarios, enabling direct searches across various data sources like search engines, databases, and knowledge graphs using LLM-generated code and query languages. It enhances retrieval by allowing direct searches and expanding the scope of data sources.\n",
      "\n",
      "2) RAG-Fusion: This component addresses traditional search limitations by employing a multi-query strategy that expands user queries into diverse perspectives. It utilizes parallel vector searches and intelligent re-ranking to uncover both explicit and transformative knowledge, improving the quality of retrieved information.\n",
      "\n",
      "3) Memory Module: Leveraging the LLM's memory, this module guides retrieval by creating an unbounded memory pool that aligns text more closely with data distribution through iterative self-enhancement. It enhances the retrieval process by improving memory-based retrieval.\n",
      "\n",
      "4) Routing: This component navigates through diverse data sources, selecting the optimal pathway for a query. It involves summarization, specific database searches, or merging different information streams to improve the efficiency of retrieval.\n",
      "\n",
      "5) Predict Module: The Predict module aims to reduce redundancy and noise by generating context directly through the LLM, ensuring relevance and accuracy in the retrieved information.\n",
      "\n",
      "6) Task Adapter Module: This module tailors RAG to various downstream tasks, automating prompt retrieval for zero-shot inputs and creating task-specific retrievers through few-shot query generation. It enhances the adaptability of RAG across different tasks and queries.\n",
      "\n",
      "Retrieved Documents:\n",
      "\n",
      "Document 1:\n",
      "becoming prevalent, supporting both sequential processing and\n",
      "integrated end-to-end training across its components. Despite\n",
      "its distinctiveness, Modular RAG builds upon the foundational\n",
      "principles of Advanced and Naive RAG, illustrating a progres-\n",
      "sion and refinement within the RAG family.\n",
      "\n",
      "Document 2:\n",
      "3https://www.langchain.com/\n",
      "C. Modular RAG\n",
      "The modular RAG architecture advances beyond the for-\n",
      "mer two RAG paradigms, offering enhanced adaptability and\n",
      "versatility. It incorporates diverse strategies for improving its\n",
      "components, such as adding a search module for similarity\n",
      "searches and refining the retriever through fine-tuning. Inno-\n",
      "vations like restructured RAG modules [13] and rearranged\n",
      "RAG pipelines [14] have been introduced to tackle specific\n",
      "challenges. The shift towards a modular RAG approach is\n",
      "becoming prevalent, supporting both sequential processing and\n",
      "integrated end-to-end training across its components. Despite\n",
      "its distinctiveness, Modular RAG builds upon the foundational\n",
      "principles of Advanced and Naive RAG, illustrating a progres-\n",
      "sion and refinement within the RAG family.\n",
      "1) New Modules: The Modular RAG framework introduces\n",
      "additional specialized components to enhance retrieval and\n",
      "processing capabilities. The Search module adapts to spe-\n",
      "cific scenarios, enabling direct searches across various data\n",
      "sources like search engines, databases, and knowledge graphs,\n",
      "using LLM-generated code and query languages [15]. RAG-\n",
      "Fusion addresses traditional search limitations by employing\n",
      "a multi-query strategy that expands user queries into diverse\n",
      "perspectives, utilizing parallel vector searches and intelligent\n",
      "\n",
      "Document 3:\n",
      "structures of Naive and Advanced RAG, characterized by a\n",
      "simple “Retrieve” and “Read” mechanism. Moreover, Modular\n",
      "RAG expands this flexibility by integrating new modules or\n",
      "adjusting interaction flow among existing ones, enhancing its\n",
      "applicability across different tasks.\n",
      "\n",
      "Document 4:\n",
      "sion and refinement within the RAG family.\n",
      "1) New Modules: The Modular RAG framework introduces\n",
      "additional specialized components to enhance retrieval and\n",
      "processing capabilities. The Search module adapts to spe-\n",
      "cific scenarios, enabling direct searches across various data\n",
      "\n",
      "Document 5:\n",
      "components, such as adding a search module for similarity\n",
      "searches and refining the retriever through fine-tuning. Inno-\n",
      "vations like restructured RAG modules [13] and rearranged\n",
      "RAG pipelines [14] have been introduced to tackle specific\n",
      "challenges. The shift towards a modular RAG approach is\n",
      "\n",
      "Document 6:\n",
      "of the information retrieved, catering to a wide array of tasks\n",
      "and queries with enhanced precision and flexibility.\n",
      "2) New Patterns: Modular RAG offers remarkable adapt-\n",
      "ability by allowing module substitution or reconfiguration\n",
      "to address specific challenges. This goes beyond the fixed\n",
      "\n",
      "Document 7:\n",
      "comprehensive approach not only streamlines the retrieval pro-\n",
      "cess but also significantly improves the quality and relevance\n",
      "of the information retrieved, catering to a wide array of tasks\n",
      "and queries with enhanced precision and flexibility.\n",
      "2) New Patterns: Modular RAG offers remarkable adapt-\n",
      "ability by allowing module substitution or reconfiguration\n",
      "to address specific challenges. This goes beyond the fixed\n",
      "structures of Naive and Advanced RAG, characterized by a\n",
      "simple “Retrieve” and “Read” mechanism. Moreover, Modular\n",
      "RAG expands this flexibility by integrating new modules or\n",
      "adjusting interaction flow among existing ones, enhancing its\n",
      "applicability across different tasks.\n",
      "Innovations such as the Rewrite-Retrieve-Read [7]model\n",
      "leverage the LLM’s capabilities to refine retrieval queries\n",
      "through a rewriting module and a LM-feedback mechanism\n",
      "to update rewriting model., improving task performance.\n",
      "Similarly, approaches like Generate-Read [13] replace tradi-\n",
      "tional retrieval with LLM-generated content, while Recite-\n",
      "Read [22] emphasizes retrieval from model weights, enhanc-\n",
      "ing the model’s ability to handle knowledge-intensive tasks.\n",
      "Hybrid retrieval strategies integrate keyword, semantic, and\n",
      "vector searches to cater to diverse queries. Additionally, em-\n",
      "ploying sub-queries and hypothetical document embeddings\n",
      "\n",
      "Document 8:\n",
      "puts to bolster another module’s functionality, illustrating a\n",
      "sophisticated understanding of enhancing module synergy.\n",
      "The flexible orchestration of Modular RAG Flow showcases\n",
      "the benefits of adaptive retrieval through techniques such as\n",
      "FLARE [24] and Self-RAG [25]. This approach transcends\n",
      "\n",
      "Document 9:\n",
      "RAG method are cost-effective and surpass the performance\n",
      "of the native LLM, they also exhibit several limitations.\n",
      "The development of Advanced RAG and Modular RAG is\n",
      "a response to these specific shortcomings in Naive RAG.\n",
      "A. Naive RAG\n",
      "The Naive RAG research paradigm represents the earli-\n",
      "\n",
      "Document 10:\n",
      "2https://www.llamaindex.ai\n",
      "3https://www.langchain.com/\n",
      "C. Modular RAG\n",
      "The modular RAG architecture advances beyond the for-\n",
      "mer two RAG paradigms, offering enhanced adaptability and\n",
      "versatility. It incorporates diverse strategies for improving its\n",
      "\n",
      "Document 11:\n",
      "within the RAG framework: Naive, Advanced, and Modu-\n",
      "lar RAG, each representing a progressive enhancement over\n",
      "its predecessors. RAG’s technical integration with other AI\n",
      "methodologies, such as fine-tuning and reinforcement learning,\n",
      "has further expanded its capabilities. Despite the progress in\n",
      "\n",
      "Document 12:\n",
      "databases. This comprehensive review paper offers a detailed\n",
      "examination of the progression of RAG paradigms, encompassing\n",
      "the Naive RAG, the Advanced RAG, and the Modular RAG.\n",
      "It meticulously scrutinizes the tripartite foundation of RAG\n",
      "\n",
      "Document 13:\n",
      "question, form a comprehensive prompt that empowers LLMs\n",
      "to generate a well-informed answer.\n",
      "The RAG research paradigm is continuously evolving, and\n",
      "we categorize it into three stages: Naive RAG, Advanced\n",
      "RAG, and Modular RAG, as showed in Figure 3. Despite\n",
      "\n",
      "Document 14:\n",
      "enhancements to the technology stack drive the development\n",
      "of RAG capabilities. RAG toolkits are converging into a\n",
      "foundational technology stack, laying the groundwork for\n",
      "advanced enterprise applications. However, a fully integrated,\n",
      "comprehensive platform concept is still in the future, requiring\n",
      "further innovation and development.\n",
      "F. Multi-modal RAG\n",
      "RAG\n",
      "has\n",
      "transcended\n",
      "its\n",
      "initial\n",
      "text-based\n",
      "question-\n",
      "answering confines, embracing a diverse array of modal data.\n",
      "This expansion has spawned innovative multimodal models\n",
      "that integrate RAG concepts across various domains:\n",
      "Image. RA-CM3 [176] stands as a pioneering multimodal\n",
      "model of both retrieving and generating text and images.\n",
      "BLIP-2 [177] leverages frozen image encoders alongside\n",
      "LLMs for efficient visual language pre-training, enabling zero-\n",
      "shot image-to-text conversions. The “Visualize Before You\n",
      "Write” method [178] employs image generation to steer the\n",
      "LM’s text generation, showing promise in open-ended text\n",
      "generation tasks.\n",
      "Audio and Video. The GSS method retrieves and stitches\n",
      "together audio clips to convert machine-translated data into\n",
      "speech-translated data [179]. UEOP marks a significant ad-\n",
      "vancement in end-to-end automatic speech recognition by\n",
      "\n",
      "Document 15:\n",
      "frameworks, which includes the retrieval, the generation and the\n",
      "augmentation techniques. The paper highlights the state-of-the-\n",
      "art technologies embedded in each of these critical components,\n",
      "providing a profound understanding of the advancements in RAG\n",
      "\n",
      "Document 16:\n",
      "Hybrid retrieval strategies integrate keyword, semantic, and\n",
      "vector searches to cater to diverse queries. Additionally, em-\n",
      "ploying sub-queries and hypothetical document embeddings\n",
      "(HyDE) [11] seeks to improve retrieval relevance by focusing\n",
      "on embedding similarities between generated answers and real\n",
      "documents.\n",
      "Adjustments in module arrangement and interaction, such\n",
      "as the Demonstrate-Search-Predict (DSP) [23] framework\n",
      "and the iterative Retrieve-Read-Retrieve-Read flow of ITER-\n",
      "RETGEN [14], showcase the dynamic use of module out-\n",
      "puts to bolster another module’s functionality, illustrating a\n",
      "sophisticated understanding of enhancing module synergy.\n",
      "The flexible orchestration of Modular RAG Flow showcases\n",
      "the benefits of adaptive retrieval through techniques such as\n",
      "FLARE [24] and Self-RAG [25]. This approach transcends\n",
      "the fixed RAG retrieval process by evaluating the necessity\n",
      "of retrieval based on different scenarios. Another benefit of\n",
      "a flexible architecture is that the RAG system can more\n",
      "easily integrate with other technologies (such as fine-tuning\n",
      "or reinforcement learning) [26]. For example, this can involve\n",
      "fine-tuning the retriever for better retrieval results, fine-tuning\n",
      "the generator for more personalized outputs, or engaging in\n",
      "collaborative fine-tuning [27].\n",
      "\n",
      "Document 17:\n",
      "analyzes the three augmentation processes. Section VI focuses\n",
      "on RAG’s downstream tasks and evaluation system. Sec-\n",
      "tion VII mainly discusses the challenges that RAG currently\n",
      "faces and its future development directions. At last, the paper\n",
      "concludes in Section VIII.\n",
      "II. OVERVIEW OF RAG\n",
      "\n",
      "Document 18:\n",
      "the other hand, involves further training the model. In the early stages of RAG (Naive RAG), there is a low demand for model modifications. As research\n",
      "progresses, Modular RAG has become more integrated with fine-tuning techniques.\n",
      "Unstructured Data, such as text, is the most widely used\n",
      "\n",
      "Document 19:\n",
      "databases. This comprehensive review paper offers a detailed\n",
      "examination of the progression of RAG paradigms, encompassing\n",
      "the Naive RAG, the Advanced RAG, and the Modular RAG.\n",
      "It meticulously scrutinizes the tripartite foundation of RAG\n",
      "frameworks, which includes the retrieval, the generation and the\n",
      "augmentation techniques. The paper highlights the state-of-the-\n",
      "art technologies embedded in each of these critical components,\n",
      "providing a profound understanding of the advancements in RAG\n",
      "systems. Furthermore, this paper introduces up-to-date evalua-\n",
      "tion framework and benchmark. At the end, this article delineates\n",
      "the challenges currently faced and points out prospective avenues\n",
      "for research and development 1.\n",
      "Index Terms—Large language model, retrieval-augmented gen-\n",
      "eration, natural language processing, information retrieval\n",
      "I. INTRODUCTION\n",
      "L\n",
      "ARGE language models (LLMs) have achieved remark-\n",
      "able success, though they still face significant limitations,\n",
      "especially in domain-specific or knowledge-intensive tasks [1],\n",
      "notably producing “hallucinations” [2] when handling queries\n",
      "beyond their training data or requiring current information. To\n",
      "overcome challenges, Retrieval-Augmented Generation (RAG)\n",
      "enhances LLMs by retrieving relevant document chunks from\n",
      "external knowledge base through semantic similarity calcu-\n",
      "\n",
      "Document 20:\n",
      "2\n",
      "Fig. 1. Technology tree of RAG research. The stages of involving RAG mainly include pre-training, fine-tuning, and inference. With the emergence of LLMs,\n",
      "research on RAG initially focused on leveraging the powerful in context learning abilities of LLMs, primarily concentrating on the inference stage. Subsequent\n",
      "research has delved deeper, gradually integrating more with the fine-tuning of LLMs. Researchers have also been exploring ways to enhance language models\n",
      "in the pre-training stage through retrieval-augmented techniques.\n",
      "advanced RAG, and modular RAG. This review contex-\n",
      "tualizes the broader scope of RAG research within the\n",
      "landscape of LLMs.\n",
      "• We identify and discuss the central technologies integral\n",
      "to the RAG process, specifically focusing on the aspects\n",
      "of “Retrieval”, “Generation” and “Augmentation”, and\n",
      "delve into their synergies, elucidating how these com-\n",
      "ponents intricately collaborate to form a cohesive and\n",
      "effective RAG framework.\n",
      "• We have summarized the current assessment methods of\n",
      "RAG, covering 26 tasks, nearly 50 datasets, outlining\n",
      "the evaluation objectives and metrics, as well as the\n",
      "current evaluation benchmarks and tools. Additionally,\n",
      "we anticipate future directions for RAG, emphasizing\n",
      "potential enhancements to tackle current challenges.\n",
      "\n",
      "Document 21:\n",
      "The summary of this paper, as depicted in Figure 6, empha-\n",
      "sizes RAG’s significant advancement in enhancing the capa-\n",
      "bilities of LLMs by integrating parameterized knowledge from\n",
      "language models with extensive non-parameterized data from\n",
      "external knowledge bases. The survey showcases the evolution\n",
      "of RAG technologies and their application on many different\n",
      "tasks. The analysis outlines three developmental paradigms\n",
      "within the RAG framework: Naive, Advanced, and Modu-\n",
      "lar RAG, each representing a progressive enhancement over\n",
      "its predecessors. RAG’s technical integration with other AI\n",
      "methodologies, such as fine-tuning and reinforcement learning,\n",
      "has further expanded its capabilities. Despite the progress in\n",
      "RAG technology, there are research opportunities to improve\n",
      "its robustness and its ability to handle extended contexts.\n",
      "RAG’s application scope is expanding into multimodal do-\n",
      "mains, adapting its principles to interpret and process diverse\n",
      "data forms like images, videos, and code. This expansion high-\n",
      "lights RAG’s significant practical implications for AI deploy-\n",
      "ment, attracting interest from academic and industrial sectors.\n",
      "17\n",
      "The growing ecosystem of RAG is evidenced by the rise in\n",
      "RAG-centric AI applications and the continuous development\n",
      "of supportive tools. As RAG’s application landscape broadens,\n",
      "there is a need to refine evaluation methodologies to keep\n",
      "\n",
      "Document 22:\n",
      "using LLM-generated code and query languages [15]. RAG-\n",
      "Fusion addresses traditional search limitations by employing\n",
      "a multi-query strategy that expands user queries into diverse\n",
      "perspectives, utilizing parallel vector searches and intelligent\n",
      "re-ranking to uncover both explicit and transformative knowl-\n",
      "edge [16]. The Memory module leverages the LLM’s memory\n",
      "to guide retrieval, creating an unbounded memory pool that\n",
      "5\n",
      "aligns the text more closely with data distribution through iter-\n",
      "ative self-enhancement [17], [18]. Routing in the RAG system\n",
      "navigates through diverse data sources, selecting the optimal\n",
      "pathway for a query, whether it involves summarization,\n",
      "specific database searches, or merging different information\n",
      "streams [19]. The Predict module aims to reduce redundancy\n",
      "and noise by generating context directly through the LLM,\n",
      "ensuring relevance and accuracy [13]. Lastly, the Task Adapter\n",
      "module tailors RAG to various downstream tasks, automating\n",
      "prompt retrieval for zero-shot inputs and creating task-specific\n",
      "retrievers through few-shot query generation [20], [21] .This\n",
      "comprehensive approach not only streamlines the retrieval pro-\n",
      "cess but also significantly improves the quality and relevance\n",
      "of the information retrieved, catering to a wide array of tasks\n",
      "and queries with enhanced precision and flexibility.\n",
      "\n",
      "Document 23:\n",
      "RAG technology, there are research opportunities to improve\n",
      "its robustness and its ability to handle extended contexts.\n",
      "RAG’s application scope is expanding into multimodal do-\n",
      "mains, adapting its principles to interpret and process diverse\n",
      "\n",
      "Document 24:\n",
      "its indexing techniques through the use of a sliding window\n",
      "approach, fine-grained segmentation, and the incorporation of\n",
      "metadata. Additionally, it incorporates several optimization\n",
      "methods to streamline the retrieval process [8].\n",
      "4\n",
      "Fig. 3.\n",
      "Comparison between the three paradigms of RAG. (Left) Naive RAG mainly consists of three parts: indexing, retrieval and generation. (Middle)\n",
      "Advanced RAG proposes multiple optimization strategies around pre-retrieval and post-retrieval, with a process similar to the Naive RAG, still following a\n",
      "chain-like structure. (Right) Modular RAG inherits and develops from the previous paradigm, showcasing greater flexibility overall. This is evident in the\n",
      "introduction of multiple specific functional modules and the replacement of existing modules. The overall process is not limited to sequential retrieval and\n",
      "generation; it includes methods such as iterative and adaptive retrieval.\n",
      "Pre-retrieval process. In this stage, the primary focus is\n",
      "on optimizing the indexing structure and the original query.\n",
      "The goal of optimizing indexing is to enhance the quality of\n",
      "the content being indexed. This involves strategies: enhancing\n",
      "data granularity, optimizing index structures, adding metadata,\n",
      "alignment optimization, and mixed retrieval. While the goal\n",
      "of query optimization is to make the user’s original question\n",
      "clearer and more suitable for the retrieval task. Common\n",
      "\n",
      "Document 25:\n",
      "comprehensive platform concept is still in the future, requiring\n",
      "further innovation and development.\n",
      "F. Multi-modal RAG\n",
      "RAG\n",
      "has\n",
      "transcended\n",
      "its\n",
      "initial\n",
      "text-based\n",
      "question-\n",
      "answering confines, embracing a diverse array of modal data.\n",
      "This expansion has spawned innovative multimodal models\n"
     ]
    }
   ],
   "source": [
    "# Path to the PDF file\n",
    "\n",
    "# Step 1: Load the documents\n",
    "docs = ArxivLoader(query='2312.10997', load_max_docs=1, load_all_available_meta=True).load()\n",
    "load_docs = docs[0].page_content\n",
    "\n",
    "# Step 2: Filter complex metadata\n",
    "data = filter_complex_metadata(docs)\n",
    "\n",
    "\n",
    "\n",
    "# Split\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "all_splits = text_splitter.split_documents(data)\n",
    "\n",
    "# Store splits\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Create a vector store with Chroma\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=embeddings)\n",
    "\n",
    "# RetrievalQA\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "\n",
    "# Set up the prompt template if needed\n",
    "prompt_template = \"\"\"\n",
    "Answer the following question based on the provided context.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "# Create the retriever with a specified top_k value\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 25})  # Set top_k to 25\n",
    "\n",
    "# Create the QA chain with the retriever and prompt\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm, retriever=retriever, chain_type_kwargs={\"prompt\": prompt}, return_source_documents=True\n",
    ")\n",
    "\n",
    "# Run a query and see the results along with the context documents\n",
    "query = \"Explain different components of Modular RAG?\"\n",
    "result = qa_chain(query)\n",
    "\n",
    "# The answer to the question\n",
    "print(\"Answer:\", result['result'])\n",
    "\n",
    "# The retrieved source documents\n",
    "print(\"\\nRetrieved Documents:\")\n",
    "for i, doc in enumerate(result['source_documents']):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(doc.page_content)  # or doc.text depending on the document structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a47df37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id='8ec5a27b-d9b7-4c7f-92e0-c07c168a904a' results=[RerankResponseResultsItem(document=None, index=1, relevance_score=0.99490017), RerankResponseResultsItem(document=None, index=23, relevance_score=0.9777563), RerankResponseResultsItem(document=None, index=3, relevance_score=0.97520185), RerankResponseResultsItem(document=None, index=18, relevance_score=0.9087799), RerankResponseResultsItem(document=None, index=15, relevance_score=0.81257004)] meta=ApiMeta(api_version=ApiMetaApiVersion(version='1', is_deprecated=None, is_experimental=None), billed_units=ApiMetaBilledUnits(input_tokens=None, output_tokens=None, search_units=1, classifications=None), tokens=None, warnings=None)\n",
      "\n",
      "Document 0:\n",
      "Relevance score on the basis of reranking is : 0.99490017\n",
      "3https://www.langchain.com/\n",
      "C. Modular RAG\n",
      "The modular RAG architecture advances beyond the for-\n",
      "mer two RAG paradigms, offering enhanced adaptability and\n",
      "versatility. It incorporates diverse strategies for improving its\n",
      "components, such as adding a search module for similarity\n",
      "searches and refining the retriever through fine-tuning. Inno-\n",
      "vations like restructured RAG modules [13] and rearranged\n",
      "RAG pipelines [14] have been introduced to tackle specific\n",
      "challenges. The shift towards a modular RAG approach is\n",
      "becoming prevalent, supporting both sequential processing and\n",
      "integrated end-to-end training across its components. Despite\n",
      "its distinctiveness, Modular RAG builds upon the foundational\n",
      "principles of Advanced and Naive RAG, illustrating a progres-\n",
      "sion and refinement within the RAG family.\n",
      "1) New Modules: The Modular RAG framework introduces\n",
      "additional specialized components to enhance retrieval and\n",
      "processing capabilities. The Search module adapts to spe-\n",
      "cific scenarios, enabling direct searches across various data\n",
      "sources like search engines, databases, and knowledge graphs,\n",
      "using LLM-generated code and query languages [15]. RAG-\n",
      "Fusion addresses traditional search limitations by employing\n",
      "a multi-query strategy that expands user queries into diverse\n",
      "perspectives, utilizing parallel vector searches and intelligent\n",
      "\n",
      "Document 1:\n",
      "Relevance score on the basis of reranking is : 0.9777563\n",
      "its indexing techniques through the use of a sliding window\n",
      "approach, fine-grained segmentation, and the incorporation of\n",
      "metadata. Additionally, it incorporates several optimization\n",
      "methods to streamline the retrieval process [8].\n",
      "4\n",
      "Fig. 3.\n",
      "Comparison between the three paradigms of RAG. (Left) Naive RAG mainly consists of three parts: indexing, retrieval and generation. (Middle)\n",
      "Advanced RAG proposes multiple optimization strategies around pre-retrieval and post-retrieval, with a process similar to the Naive RAG, still following a\n",
      "chain-like structure. (Right) Modular RAG inherits and develops from the previous paradigm, showcasing greater flexibility overall. This is evident in the\n",
      "introduction of multiple specific functional modules and the replacement of existing modules. The overall process is not limited to sequential retrieval and\n",
      "generation; it includes methods such as iterative and adaptive retrieval.\n",
      "Pre-retrieval process. In this stage, the primary focus is\n",
      "on optimizing the indexing structure and the original query.\n",
      "The goal of optimizing indexing is to enhance the quality of\n",
      "the content being indexed. This involves strategies: enhancing\n",
      "data granularity, optimizing index structures, adding metadata,\n",
      "alignment optimization, and mixed retrieval. While the goal\n",
      "of query optimization is to make the user’s original question\n",
      "clearer and more suitable for the retrieval task. Common\n",
      "\n",
      "Document 2:\n",
      "Relevance score on the basis of reranking is : 0.97520185\n",
      "sion and refinement within the RAG family.\n",
      "1) New Modules: The Modular RAG framework introduces\n",
      "additional specialized components to enhance retrieval and\n",
      "processing capabilities. The Search module adapts to spe-\n",
      "cific scenarios, enabling direct searches across various data\n",
      "\n",
      "Document 3:\n",
      "Relevance score on the basis of reranking is : 0.9087799\n",
      "databases. This comprehensive review paper offers a detailed\n",
      "examination of the progression of RAG paradigms, encompassing\n",
      "the Naive RAG, the Advanced RAG, and the Modular RAG.\n",
      "It meticulously scrutinizes the tripartite foundation of RAG\n",
      "frameworks, which includes the retrieval, the generation and the\n",
      "augmentation techniques. The paper highlights the state-of-the-\n",
      "art technologies embedded in each of these critical components,\n",
      "providing a profound understanding of the advancements in RAG\n",
      "systems. Furthermore, this paper introduces up-to-date evalua-\n",
      "tion framework and benchmark. At the end, this article delineates\n",
      "the challenges currently faced and points out prospective avenues\n",
      "for research and development 1.\n",
      "Index Terms—Large language model, retrieval-augmented gen-\n",
      "eration, natural language processing, information retrieval\n",
      "I. INTRODUCTION\n",
      "L\n",
      "ARGE language models (LLMs) have achieved remark-\n",
      "able success, though they still face significant limitations,\n",
      "especially in domain-specific or knowledge-intensive tasks [1],\n",
      "notably producing “hallucinations” [2] when handling queries\n",
      "beyond their training data or requiring current information. To\n",
      "overcome challenges, Retrieval-Augmented Generation (RAG)\n",
      "enhances LLMs by retrieving relevant document chunks from\n",
      "external knowledge base through semantic similarity calcu-\n",
      "\n",
      "Document 4:\n",
      "Relevance score on the basis of reranking is : 0.81257004\n",
      "Hybrid retrieval strategies integrate keyword, semantic, and\n",
      "vector searches to cater to diverse queries. Additionally, em-\n",
      "ploying sub-queries and hypothetical document embeddings\n",
      "(HyDE) [11] seeks to improve retrieval relevance by focusing\n",
      "on embedding similarities between generated answers and real\n",
      "documents.\n",
      "Adjustments in module arrangement and interaction, such\n",
      "as the Demonstrate-Search-Predict (DSP) [23] framework\n",
      "and the iterative Retrieve-Read-Retrieve-Read flow of ITER-\n",
      "RETGEN [14], showcase the dynamic use of module out-\n",
      "puts to bolster another module’s functionality, illustrating a\n",
      "sophisticated understanding of enhancing module synergy.\n",
      "The flexible orchestration of Modular RAG Flow showcases\n",
      "the benefits of adaptive retrieval through techniques such as\n",
      "FLARE [24] and Self-RAG [25]. This approach transcends\n",
      "the fixed RAG retrieval process by evaluating the necessity\n",
      "of retrieval based on different scenarios. Another benefit of\n",
      "a flexible architecture is that the RAG system can more\n",
      "easily integrate with other technologies (such as fine-tuning\n",
      "or reinforcement learning) [26]. For example, this can involve\n",
      "fine-tuning the retriever for better retrieval results, fine-tuning\n",
      "the generator for more personalized outputs, or engaging in\n",
      "collaborative fine-tuning [27].\n"
     ]
    }
   ],
   "source": [
    "# Re-Rank them with cohere\n",
    "import cohere\n",
    "# Get your cohere API key on: www.cohere.com\n",
    "co = cohere.Client(f\"{os.environ['COHERE_API_KEY']}\")\n",
    "docs = [doc.page_content for doc in result['source_documents']]\n",
    "\n",
    "# Re-Rank them with cohere\n",
    "top_n=5\n",
    "rerank_hits = co.rerank(query=query, documents=docs, top_n=top_n, model='rerank-multilingual-v3.0')\n",
    "print(rerank_hits)\n",
    "#[doc[rerank_hits.results[i].index] for i in range(5)]\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    if i>top_n-1:\n",
    "        break\n",
    "    else:\n",
    "        print(f\"\\nDocument {i}:\")\n",
    "        print(f\"Relevance score on the basis of reranking is : {rerank_hits.results[i].relevance_score}\") \n",
    "        print(docs[rerank_hits.results[i].index])  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c2bdc4",
   "metadata": {},
   "source": [
    "###  Splitting Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20a2dd70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Character Text Splitting ####\n",
      "[Document(page_content='In 2024, India will find itself at ', metadata={'source': 'local'}), Document(page_content='the center of global attention, hos', metadata={'source': 'local'}), Document(page_content='ting both the highly anticipated ge', metadata={'source': 'local'}), Document(page_content='neral elections and the Cricket Wor', metadata={'source': 'local'}), Document(page_content='ld Cup. On the political front, the', metadata={'source': 'local'}), Document(page_content=' general elections will see over 90', metadata={'source': 'local'}), Document(page_content='0 million eligible voters making th', metadata={'source': 'local'}), Document(page_content='eir voices heard in a democratic ex', metadata={'source': 'local'}), Document(page_content='ercise unparalleled in scale. \\nPoli', metadata={'source': 'local'}), Document(page_content='tical parties are already mobilizin', metadata={'source': 'local'}), Document(page_content='g their bases, with campaigns focus', metadata={'source': 'local'}), Document(page_content='ed on critical issues like economic', metadata={'source': 'local'}), Document(page_content=' growth, social justice, and nation', metadata={'source': 'local'}), Document(page_content='al security. Meanwhile, cricket fev', metadata={'source': 'local'}), Document(page_content='er will grip the nation as teams fr', metadata={'source': 'local'}), Document(page_content='om around the world compete for glo', metadata={'source': 'local'}), Document(page_content='ry in the ICC Cricket World Cup. \\nS', metadata={'source': 'local'}), Document(page_content='tadia will roar with the cheers of ', metadata={'source': 'local'}), Document(page_content='passionate fans, and cricket pitche', metadata={'source': 'local'}), Document(page_content='s will become the stage for thrilli', metadata={'source': 'local'}), Document(page_content='ng displays of skill and sportsmans', metadata={'source': 'local'}), Document(page_content='hip. \\nAs politicians rally for vote', metadata={'source': 'local'}), Document(page_content='s and cricketers battle for the cha', metadata={'source': 'local'}), Document(page_content='mpionship, these parallel events wi', metadata={'source': 'local'}), Document(page_content='ll underscore the dual fervor that ', metadata={'source': 'local'}), Document(page_content=\"defines India's national identity: \", metadata={'source': 'local'}), Document(page_content='a deep commitment to democracy and ', metadata={'source': 'local'}), Document(page_content='an unbridled love for cricket\\n', metadata={'source': 'local'})]\n",
      "[Document(page_content='In 2024, India will find itself at '), Document(page_content='the center of global attention, hos'), Document(page_content='ting both the highly anticipated ge'), Document(page_content='neral elections and the Cricket Wor'), Document(page_content='ld Cup. On the political front, the'), Document(page_content=' general elections will see over 90'), Document(page_content='0 million eligible voters making th'), Document(page_content='eir voices heard in a democratic ex'), Document(page_content='ercise unparalleled in scale. \\nPoli'), Document(page_content='tical parties are already mobilizin'), Document(page_content='g their bases, with campaigns focus'), Document(page_content='ed on critical issues like economic'), Document(page_content=' growth, social justice, and nation'), Document(page_content='al security. Meanwhile, cricket fev'), Document(page_content='er will grip the nation as teams fr'), Document(page_content='om around the world compete for glo'), Document(page_content='ry in the ICC Cricket World Cup. \\nS'), Document(page_content='tadia will roar with the cheers of '), Document(page_content='passionate fans, and cricket pitche'), Document(page_content='s will become the stage for thrilli'), Document(page_content='ng displays of skill and sportsmans'), Document(page_content='hip. \\nAs politicians rally for vote'), Document(page_content='s and cricketers battle for the cha'), Document(page_content='mpionship, these parallel events wi'), Document(page_content='ll underscore the dual fervor that '), Document(page_content=\"defines India's national identity: \"), Document(page_content='a deep commitment to democracy and '), Document(page_content='an unbridled love for cricket\\n')]\n"
     ]
    }
   ],
   "source": [
    "# 1. Character Text Splitting\n",
    "print(\"#### Character Text Splitting ####\")\n",
    "\n",
    "text = \"\"\"In 2024, India will find itself at the center of global attention, hosting both the highly anticipated general elections and the Cricket World Cup. On the political front, the general elections will see over 900 million eligible voters making their voices heard in a democratic exercise unparalleled in scale. \n",
    "Political parties are already mobilizing their bases, with campaigns focused on critical issues like economic growth, social justice, and national security. Meanwhile, cricket fever will grip the nation as teams from around the world compete for glory in the ICC Cricket World Cup. \n",
    "Stadia will roar with the cheers of passionate fans, and cricket pitches will become the stage for thrilling displays of skill and sportsmanship. \n",
    "As politicians rally for votes and cricketers battle for the championship, these parallel events will underscore the dual fervor that defines India's national identity: a deep commitment to democracy and an unbridled love for cricket\n",
    "\"\"\"\n",
    "# Manual Splitting\n",
    "chunks = []\n",
    "chunk_size = 35 # Characters\n",
    "for i in range(0, len(text), chunk_size):\n",
    "    chunk = text[i:i + chunk_size]\n",
    "    chunks.append(chunk)\n",
    "documents = [Document(page_content=chunk, metadata={\"source\": \"local\"}) for chunk in chunks]\n",
    "print(documents)\n",
    "\n",
    "# Automatic Text Splitting\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(chunk_size = 35, chunk_overlap=0, separator='', strip_whitespace=False)\n",
    "documents = text_splitter.create_documents([text])\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "204f9cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Recursive Character Text Splitting ####\n",
      "[Document(page_content='In 2024, India will find itself at the center of global'), Document(page_content='attention, hosting both the highly anticipated general elections'), Document(page_content='and the Cricket World Cup. On the political front, the general'), Document(page_content='elections will see over 900 million eligible voters making their'), Document(page_content='voices heard in a democratic exercise unparalleled in scale.'), Document(page_content='Political parties are already mobilizing their bases, with'), Document(page_content='campaigns focused on critical issues like economic growth,'), Document(page_content='social justice, and national security. Meanwhile, cricket fever'), Document(page_content='will grip the nation as teams from around the world compete for'), Document(page_content='glory in the ICC Cricket World Cup.'), Document(page_content='Stadia will roar with the cheers of passionate fans, and cricket'), Document(page_content='pitches will become the stage for thrilling displays of skill'), Document(page_content='and sportsmanship.'), Document(page_content='As politicians rally for votes and cricketers battle for the'), Document(page_content='championship, these parallel events will underscore the dual'), Document(page_content=\"fervor that defines India's national identity: a deep commitment\"), Document(page_content='to democracy and an unbridled love for cricket')]\n"
     ]
    }
   ],
   "source": [
    "# 2. Recursive Character Text Splitting\n",
    "print(\"#### Recursive Character Text Splitting ####\")\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 65, chunk_overlap=0) # [\"\\n\\n\", \"\\n\", \" \", \"\"] 65,450\n",
    "print(text_splitter.create_documents([text])) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21b15eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Document Specific Splitting ####\n",
      "[Document(page_content='In 2024, India will find itself at the'), Document(page_content='center of global attention, hosting'), Document(page_content='both the highly anticipated general'), Document(page_content='elections and the Cricket World Cup. On'), Document(page_content='the political front, the general'), Document(page_content='elections will see over 900 million'), Document(page_content='eligible voters making their voices'), Document(page_content='heard in a democratic exercise'), Document(page_content='unparalleled in scale.'), Document(page_content='Political parties are already'), Document(page_content='mobilizing their bases, with campaigns'), Document(page_content='focused on critical issues like'), Document(page_content='economic growth, social justice, and'), Document(page_content='national security. Meanwhile, cricket'), Document(page_content='fever will grip the nation as teams'), Document(page_content='from around the world compete for glory'), Document(page_content='in the ICC Cricket World Cup.'), Document(page_content='Stadia will roar with the cheers of'), Document(page_content='passionate fans, and cricket pitches'), Document(page_content='will become the stage for thrilling'), Document(page_content='displays of skill and sportsmanship.'), Document(page_content='As politicians rally for votes and'), Document(page_content='cricketers battle for the championship,'), Document(page_content='these parallel events will underscore'), Document(page_content=\"the dual fervor that defines India's\"), Document(page_content='national identity: a deep commitment to'), Document(page_content='democracy and an unbridled love for'), Document(page_content='cricket')]\n",
      "[Document(page_content='class Person:\\n  def __init__(self, name, age):\\n    self.name = name\\n    self.age = age'), Document(page_content='p1 = Person(\"John\", 36)\\n\\nfor i in range(10):\\n    print (i)')]\n",
      "[Document(page_content='// Function is called, the return value will end up in x'), Document(page_content='let x = myFunction(4, 3);'), Document(page_content='function myFunction(a, b) {'), Document(page_content='// Function returns the product of a and b\\n  return a * b;\\n}')]\n"
     ]
    }
   ],
   "source": [
    "# 3. Document Specific Splitting\n",
    "print(\"#### Document Specific Splitting ####\")\n",
    "\n",
    "# Document Specific Splitting - Markdown\n",
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "splitter = MarkdownTextSplitter(chunk_size = 40, chunk_overlap=0)\n",
    "markdown_text = text\n",
    "print(splitter.create_documents([markdown_text]))\n",
    "\n",
    "# Document Specific Splitting - Python\n",
    "from langchain.text_splitter import PythonCodeTextSplitter\n",
    "python_text = \"\"\"\n",
    "class Person:\n",
    "  def __init__(self, name, age):\n",
    "    self.name = name\n",
    "    self.age = age\n",
    "\n",
    "p1 = Person(\"John\", 36)\n",
    "\n",
    "for i in range(10):\n",
    "    print (i)\n",
    "\"\"\"\n",
    "python_splitter = PythonCodeTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "print(python_splitter.create_documents([python_text]))\n",
    "\n",
    "# Document Specific Splitting - Javascript\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, Language\n",
    "javascript_text = \"\"\"\n",
    "// Function is called, the return value will end up in x\n",
    "let x = myFunction(4, 3);\n",
    "\n",
    "function myFunction(a, b) {\n",
    "// Function returns the product of a and b\n",
    "  return a * b;\n",
    "}\n",
    "\"\"\"\n",
    "js_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.JS, chunk_size=65, chunk_overlap=0\n",
    ")\n",
    "print(js_splitter.create_documents([javascript_text]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f1ec9b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Semantic Chunking ####\n",
      "[Document(page_content='In 2024, India will find itself at the center of global attention, hosting both the highly anticipated general elections and the Cricket World Cup. On the political front, the general elections will see over 900 million eligible voters making their voices heard in a democratic exercise unparalleled in scale. Political parties are already mobilizing their bases, with campaigns focused on critical issues like economic growth, social justice, and national security. Meanwhile, cricket fever will grip the nation as teams from around the world compete for glory in the ICC Cricket World Cup.'), Document(page_content=\"Stadia will roar with the cheers of passionate fans, and cricket pitches will become the stage for thrilling displays of skill and sportsmanship. As politicians rally for votes and cricketers battle for the championship, these parallel events will underscore the dual fervor that defines India's national identity: a deep commitment to democracy and an unbridled love for cricket\\n\")]\n"
     ]
    }
   ],
   "source": [
    "# 4. Semantic Chunking\n",
    "print(\"#### Semantic Chunking ####\")\n",
    "\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Percentile - all differences between sentences are calculated, and then any difference greater than the X percentile is split\n",
    "text_splitter = SemanticChunker(OpenAIEmbeddings())\n",
    "text_splitter = SemanticChunker(\n",
    "    OpenAIEmbeddings(), breakpoint_threshold_type=\"percentile\" # \"standard_deviation\", \"interquartile\"\n",
    ")\n",
    "documents = text_splitter.create_documents([text])\n",
    "print(documents)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
